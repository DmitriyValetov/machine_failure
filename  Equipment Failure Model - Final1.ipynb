{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "## Install all of the relevant Python Libraries", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "!pip uninstall watson-machine-learning-client -y\n!pip uninstall numpy -y\n!pip uninstall lime  -y\n!pip uninstall SciPy -y\n!pip uninstall imblearn -y\n!pip uninstall plotly -y\n!pip uninstall imblearn -y", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "!pip install --upgrade watson-machine-learning-client --no-cache \n!pip install --upgrade numpy --no-cache \n!pip install --upgrade lime --no-cache \n!pip install --upgrade SciPy --no-cache \n!pip install imblearn --upgrade", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Collecting watson-machine-learning-client\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/a1/c503614455fb734b0989e8d6abaf24d0544d7370f7eb2b80ffbc99a40caf/watson_machine_learning_client-1.0.371-py3-none-any.whl (536kB)\n\u001b[K    100% |################################| 542kB 67.3MB/s ta 0:00:01\n\u001b[?25hCollecting urllib3 (from watson-machine-learning-client)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/60/247f23a7121ae632d62811ba7f273d0e58972d75e58a94d329d51550a47d/urllib3-1.25.3-py2.py3-none-any.whl (150kB)\n\u001b[K    100% |################################| 153kB 66.2MB/s ta 0:00:01\n\u001b[?25hCollecting tqdm (from watson-machine-learning-client)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/3d/7a6b68b631d2ab54975f3a4863f3c4e9b26445353264ef01f465dc9b0208/tqdm-4.32.2-py2.py3-none-any.whl (50kB)\n\u001b[K    100% |################################| 51kB 45.4MB/s ta 0:00:01\n\u001b[?25hCollecting certifi (from watson-machine-learning-client)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/1b/b853c7a9d4f6a6d00749e94eb6f3a041e342a885b87340b79c1ef73e3a78/certifi-2019.6.16-py2.py3-none-any.whl (157kB)\n\u001b[K    100% |################################| 163kB 67.8MB/s ta 0:00:01\n\u001b[?25hCollecting lomond (from watson-machine-learning-client)\n  Downloading https://files.pythonhosted.org/packages/0f/b1/02eebed49c754b01b17de7705caa8c4ceecfb4f926cdafc220c863584360/lomond-0.3.3-py2.py3-none-any.whl\nCollecting tabulate (from watson-machine-learning-client)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c2/fd/202954b3f0eb896c53b7b6f07390851b1fd2ca84aa95880d7ae4f434c4ac/tabulate-0.8.3.tar.gz (46kB)\n\u001b[K    100% |################################| 51kB 47.4MB/s ta 0:00:01\n\u001b[?25hCollecting pandas (from watson-machine-learning-client)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/9a/7eb9952f4b4d73fbd75ad1d5d6112f407e695957444cb695cbb3cdab918a/pandas-0.25.0-cp36-cp36m-manylinux1_x86_64.whl (10.5MB)\n\u001b[K    100% |################################| 10.5MB 50.2MB/s ta 0:00:01\n\u001b[?25hCollecting requests (from watson-machine-learning-client)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl (57kB)\n\u001b[K    100% |################################| 61kB 28.9MB/s ta 0:00:01\n\u001b[?25hCollecting ibm-cos-sdk (from watson-machine-learning-client)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/fd/7f00531e462bdef3b51a080b9efb2d32389a07fd73dc3ac1ceb27e247d6b/ibm-cos-sdk-2.5.2.tar.gz (52kB)\n\u001b[K    100% |################################| 61kB 29.6MB/s ta 0:00:01\n\u001b[?25hCollecting six>=1.10.0 (from lomond->watson-machine-learning-client)\n  Downloading https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\nCollecting pytz>=2017.2 (from pandas->watson-machine-learning-client)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/76/46d697698a143e05f77bec5a526bf4e56a0be61d63425b68f4ba553b51f2/pytz-2019.2-py2.py3-none-any.whl (508kB)\n\u001b[K    100% |################################| 512kB 38.2MB/s ta 0:00:01\n\u001b[?25hCollecting python-dateutil>=2.6.1 (from pandas->watson-machine-learning-client)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/17/c62faccbfbd163c7f57f3844689e3a78bae1f403648a6afb1d0866d87fbb/python_dateutil-2.8.0-py2.py3-none-any.whl (226kB)\n\u001b[K    100% |################################| 235kB 38.3MB/s ta 0:00:01\n\u001b[?25hCollecting numpy>=1.13.3 (from pandas->watson-machine-learning-client)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/b9/bda9781f0a74b90ebd2e046fde1196182900bd4a8e1ea503d3ffebc50e7c/numpy-1.17.0-cp36-cp36m-manylinux1_x86_64.whl (20.4MB)\n\u001b[K    100% |################################| 20.4MB 67.6MB/s ta 0:00:01\n\u001b[?25hCollecting idna<2.9,>=2.5 (from requests->watson-machine-learning-client)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl (58kB)\n\u001b[K    100% |################################| 61kB 50.0MB/s ta 0:00:01\n\u001b[?25hCollecting chardet<3.1.0,>=3.0.2 (from requests->watson-machine-learning-client)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl (133kB)\n\u001b[K    100% |################################| 143kB 68.1MB/s ta 0:00:01\n\u001b[?25hCollecting ibm-cos-sdk-core>=2.0.0 (from ibm-cos-sdk->watson-machine-learning-client)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/ef/036e9cdb65f6d126e23555b82cddca36e8f5ab70728167c18b9a2dc4ba74/ibm-cos-sdk-core-2.5.2.tar.gz (1.1MB)\n\u001b[K    100% |################################| 1.1MB 67.3MB/s ta 0:00:01\n\u001b[?25hCollecting ibm-cos-sdk-s3transfer>=2.0.0 (from ibm-cos-sdk->watson-machine-learning-client)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/b7/1bf1792978a65668af1fe00887c65ffffa02b784b779f6392a707b045d33/ibm-cos-sdk-s3transfer-2.5.2.tar.gz (134kB)\n\u001b[K    100% |################################| 143kB 69.2MB/s ta 0:00:01\n\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1 (from ibm-cos-sdk->watson-machine-learning-client)\n  Downloading https://files.pythonhosted.org/packages/83/94/7179c3832a6d45b266ddb2aac329e101367fbdb11f425f13771d27f225bb/jmespath-0.9.4-py2.py3-none-any.whl\nCollecting docutils>=0.10 (from ibm-cos-sdk-core>=2.0.0->ibm-cos-sdk->watson-machine-learning-client)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/cd/a6aa959dca619918ccb55023b4cb151949c64d4d5d55b3f4ffd7eee0c6e8/docutils-0.15.2-py3-none-any.whl (547kB)\n\u001b[K    100% |################################| 552kB 70.8MB/s ta 0:00:01\n\u001b[31mtensorflow 1.13.1 requires tensorboard<1.14.0,>=1.13.0, which is not installed.\u001b[0m\n\u001b[31mspyder 3.3.3 requires pyqt5<=5.12; python_version >= \"3\", which is not installed.\u001b[0m\n\u001b[31mibm-cos-sdk-core 2.5.2 has requirement urllib3<1.25,>=1.20, but you'll have urllib3 1.25.3 which is incompatible.\u001b[0m\n\u001b[31mbotocore 1.12.82 has requirement urllib3<1.25,>=1.20, but you'll have urllib3 1.25.3 which is incompatible.\u001b[0m\n\u001b[?25hInstalling collected packages: urllib3, tqdm, certifi, six, lomond, tabulate, pytz, python-dateutil, numpy, pandas, idna, chardet, requests, jmespath, docutils, ibm-cos-sdk-core, ibm-cos-sdk-s3transfer, ibm-cos-sdk, watson-machine-learning-client\n  Running setup.py install for tabulate ... \u001b[?25ldone\n\u001b[?25h  Running setup.py install for ibm-cos-sdk-core ... \u001b[?25ldone\n\u001b[?25h  Running setup.py install for ibm-cos-sdk-s3transfer ... \u001b[?25ldone\n\u001b[?25h  Running setup.py install for ibm-cos-sdk ... \u001b[?25ldone\n\u001b[?25hSuccessfully installed certifi-2019.6.16 chardet-3.0.4 docutils-0.15.2 ibm-cos-sdk-2.5.2 ibm-cos-sdk-core-2.5.2 ibm-cos-sdk-s3transfer-2.5.2 idna-2.8 jmespath-0.9.4 lomond-0.3.3 numpy-1.17.0 pandas-0.25.0 python-dateutil-2.8.0 pytz-2019.2 requests-2.22.0 six-1.12.0 tabulate-0.8.3 tqdm-4.32.2 urllib3-1.25.3 watson-machine-learning-client-1.0.371\n\u001b[31mException:\nTraceback (most recent call last):\n  File \"/opt/ibm/conda/miniconda36/lib/python3.6/site-packages/pip/_internal/cli/base_command.py\", line 176, in main\n    status = self.run(options, args)\n  File \"/opt/ibm/conda/miniconda36/lib/python3.6/site-packages/pip/_internal/commands/install.py\", line 441, in run\n    options.target_dir, target_temp_dir, options.upgrade\n  File \"/opt/ibm/conda/miniconda36/lib/python3.6/site-packages/pip/_internal/commands/install.py\", line 492, in _handle_target_dir\n    shutil.rmtree(target_item_dir)\n  File \"/opt/ibm/conda/miniconda36/lib/python3.6/shutil.py\", line 486, in rmtree\n    _rmtree_safe_fd(fd, path, onerror)\n  File \"/opt/ibm/conda/miniconda36/lib/python3.6/shutil.py\", line 428, in _rmtree_safe_fd\n    onerror(os.rmdir, fullname, sys.exc_info())\n  File \"/opt/ibm/conda/miniconda36/lib/python3.6/shutil.py\", line 426, in _rmtree_safe_fd\n    os.rmdir(name, dir_fd=topfd)\nOSError: [Errno 39] Directory not empty: 'random'\u001b[0m\nCollecting numpy\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/b9/bda9781f0a74b90ebd2e046fde1196182900bd4a8e1ea503d3ffebc50e7c/numpy-1.17.0-cp36-cp36m-manylinux1_x86_64.whl (20.4MB)\n"
                }, 
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "\u001b[K    100% |################################| 20.4MB 68.8MB/s ta 0:00:01\n\u001b[31mtensorflow 1.13.1 requires tensorboard<1.14.0,>=1.13.0, which is not installed.\u001b[0m\n\u001b[?25hInstalling collected packages: numpy\nSuccessfully installed numpy-1.17.0\n\u001b[31mException:\nTraceback (most recent call last):\n  File \"/opt/ibm/conda/miniconda36/lib/python3.6/site-packages/pip/_internal/cli/base_command.py\", line 176, in main\n    status = self.run(options, args)\n  File \"/opt/ibm/conda/miniconda36/lib/python3.6/site-packages/pip/_internal/commands/install.py\", line 441, in run\n    options.target_dir, target_temp_dir, options.upgrade\n  File \"/opt/ibm/conda/miniconda36/lib/python3.6/site-packages/pip/_internal/commands/install.py\", line 492, in _handle_target_dir\n    shutil.rmtree(target_item_dir)\n  File \"/opt/ibm/conda/miniconda36/lib/python3.6/shutil.py\", line 486, in rmtree\n    _rmtree_safe_fd(fd, path, onerror)\n  File \"/opt/ibm/conda/miniconda36/lib/python3.6/shutil.py\", line 424, in _rmtree_safe_fd\n    _rmtree_safe_fd(dirfd, fullname, onerror)\n  File \"/opt/ibm/conda/miniconda36/lib/python3.6/shutil.py\", line 444, in _rmtree_safe_fd\n    onerror(os.unlink, fullname, sys.exc_info())\n  File \"/opt/ibm/conda/miniconda36/lib/python3.6/shutil.py\", line 442, in _rmtree_safe_fd\n    os.unlink(name, dir_fd=topfd)\nOSError: [Errno 16] Device or resource busy: '.nfs00000000028595eb00000153'\u001b[0m\nCollecting lime\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/72/4be533df5151fcb48942515e95e88281ec439396c48d67d3ae41f27586f0/lime-0.1.1.36.tar.gz (275kB)\n\u001b[K    100% |################################| 276kB 5.6MB/s ta 0:00:01\n\u001b[?25hCollecting numpy (from lime)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/b9/bda9781f0a74b90ebd2e046fde1196182900bd4a8e1ea503d3ffebc50e7c/numpy-1.17.0-cp36-cp36m-manylinux1_x86_64.whl (20.4MB)\n\u001b[K    100% |################################| 20.4MB 70.6MB/s ta 0:00:01\n\u001b[?25hCollecting scipy (from lime)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/4c/5f81e7264b0a7a8bd570810f48cd346ba36faedbd2ba255c873ad556de76/scipy-1.3.0-cp36-cp36m-manylinux1_x86_64.whl (25.2MB)\n\u001b[K    100% |################################| 25.2MB 66.6MB/s ta 0:00:01\n\u001b[?25hCollecting scikit-learn>=0.18 (from lime)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/c5/d2238762d780dde84a20b8c761f563fe882b88c5a5fb03c056547c442a19/scikit_learn-0.21.3-cp36-cp36m-manylinux1_x86_64.whl (6.7MB)\n\u001b[K    100% |################################| 6.7MB 68.8MB/s ta 0:00:01\n\u001b[?25hCollecting matplotlib (from lime)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/57/4f/dd381ecf6c6ab9bcdaa8ea912e866dedc6e696756156d8ecc087e20817e2/matplotlib-3.1.1-cp36-cp36m-manylinux1_x86_64.whl (13.1MB)\n\u001b[K    100% |################################| 13.1MB 67.7MB/s ta 0:00:01\n\u001b[?25hCollecting scikit-image>=0.12 (from lime)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/ab/674e168bf7d0bc597218b3bec858d02c23fbac9ec1fec9cad878c6cee95f/scikit_image-0.15.0-cp36-cp36m-manylinux1_x86_64.whl (26.3MB)\n\u001b[K    100% |################################| 26.3MB 69.8MB/s ta 0:00:01\n\u001b[?25hCollecting joblib>=0.11 (from scikit-learn>=0.18->lime)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/c1/50a758e8247561e58cb87305b1e90b171b8c767b15b12a1734001f41d356/joblib-0.13.2-py2.py3-none-any.whl (278kB)\n\u001b[K    100% |################################| 286kB 67.0MB/s ta 0:00:01\n\u001b[?25hCollecting kiwisolver>=1.0.1 (from matplotlib->lime)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/a1/5742b56282449b1c0968197f63eae486eca2c35dcd334bab75ad524e0de1/kiwisolver-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (90kB)\n\u001b[K    100% |################################| 92kB 53.7MB/s ta 0:00:01\n\u001b[?25hCollecting python-dateutil>=2.1 (from matplotlib->lime)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/17/c62faccbfbd163c7f57f3844689e3a78bae1f403648a6afb1d0866d87fbb/python_dateutil-2.8.0-py2.py3-none-any.whl (226kB)\n\u001b[K    100% |################################| 235kB 69.6MB/s ta 0:00:01\n\u001b[?25hCollecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->lime)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/fa/0160cd525c62d7abd076a070ff02b2b94de589f1a9789774f17d7c54058e/pyparsing-2.4.2-py2.py3-none-any.whl (65kB)\n\u001b[K    100% |################################| 71kB 52.4MB/s ta 0:00:01\n\u001b[?25hCollecting cycler>=0.10 (from matplotlib->lime)\n  Downloading https://files.pythonhosted.org/packages/f7/d2/e07d3ebb2bd7af696440ce7e754c59dd546ffe1bbe732c8ab68b9c834e61/cycler-0.10.0-py2.py3-none-any.whl\nCollecting networkx>=2.0 (from scikit-image>=0.12->lime)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/08/f20aef11d4c343b557e5de6b9548761811eb16e438cee3d32b1c66c8566b/networkx-2.3.zip (1.7MB)\n\u001b[K    100% |################################| 1.8MB 67.7MB/s ta 0:00:01\n\u001b[?25hCollecting imageio>=2.0.1 (from scikit-image>=0.12->lime)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0a/943c965d372dae0b1f1482677d29030ab834351a61a9a632fd62f27f1523/imageio-2.5.0-py3-none-any.whl (3.3MB)\n\u001b[K    100% |################################| 3.3MB 69.1MB/s ta 0:00:01\n\u001b[?25hCollecting PyWavelets>=0.4.0 (from scikit-image>=0.12->lime)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4e/cd/528dba0b474b08f6f9a3a5e1b4bb23d8e33ed5d9f0e321cc967c2607df05/PyWavelets-1.0.3-cp36-cp36m-manylinux1_x86_64.whl (4.4MB)\n\u001b[K    100% |################################| 4.4MB 71.2MB/s ta 0:00:01\n\u001b[?25hCollecting pillow>=4.3.0 (from scikit-image>=0.12->lime)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/41/db6dec65ddbc176a59b89485e8cc136a433ed9c6397b6bfe2cd38412051e/Pillow-6.1.0-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n\u001b[K    100% |################################| 2.1MB 68.2MB/s ta 0:00:01\n\u001b[?25hCollecting setuptools (from kiwisolver>=1.0.1->matplotlib->lime)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/51/f45cea425fd5cb0b0380f5b0f048ebc1da5b417e48d304838c02d6288a1e/setuptools-41.0.1-py2.py3-none-any.whl (575kB)\n\u001b[K    100% |################################| 583kB 68.6MB/s ta 0:00:01\n\u001b[?25hCollecting six>=1.5 (from python-dateutil>=2.1->matplotlib->lime)\n  Downloading https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\nCollecting decorator>=4.3.0 (from networkx>=2.0->scikit-image>=0.12->lime)\n  Downloading https://files.pythonhosted.org/packages/5f/88/0075e461560a1e750a0dcbf77f1d9de775028c37a19a346a6c565a257399/decorator-4.4.0-py2.py3-none-any.whl\n\u001b[31mtensorflow 1.13.1 requires tensorboard<1.14.0,>=1.13.0, which is not installed.\u001b[0m\n\u001b[31mibm-cos-sdk-core 2.5.2 has requirement urllib3<1.25,>=1.20, but you'll have urllib3 1.25.3 which is incompatible.\u001b[0m\n\u001b[31mbotocore 1.12.82 has requirement urllib3<1.25,>=1.20, but you'll have urllib3 1.25.3 which is incompatible.\u001b[0m\nInstalling collected packages: numpy, scipy, joblib, scikit-learn, setuptools, kiwisolver, six, python-dateutil, pyparsing, cycler, matplotlib, decorator, networkx, pillow, imageio, PyWavelets, scikit-image, lime\n  Running setup.py install for networkx ... \u001b[?25ldone\n\u001b[?25h  Running setup.py install for lime ... \u001b[?25ldone\n\u001b[?25hSuccessfully installed PyWavelets-1.0.3 cycler-0.10.0 decorator-4.4.0 imageio-2.5.0 joblib-0.13.2 kiwisolver-1.1.0 lime-0.1.1.36 matplotlib-3.1.1 networkx-2.3 numpy-1.17.0 pillow-6.1.0 pyparsing-2.4.2 python-dateutil-2.8.0 scikit-image-0.15.0 scikit-learn-0.21.3 scipy-1.3.0 setuptools-41.0.1 six-1.12.0\n\u001b[31mException:\nTraceback (most recent call last):\n  File \"/opt/ibm/conda/miniconda36/lib/python3.6/site-packages/pip/_internal/cli/base_command.py\", line 176, in main\n    status = self.run(options, args)\n  File \"/opt/ibm/conda/miniconda36/lib/python3.6/site-packages/pip/_internal/commands/install.py\", line 441, in run\n    options.target_dir, target_temp_dir, options.upgrade\n  File \"/opt/ibm/conda/miniconda36/lib/python3.6/site-packages/pip/_internal/commands/install.py\", line 492, in _handle_target_dir\n    shutil.rmtree(target_item_dir)\n  File \"/opt/ibm/conda/miniconda36/lib/python3.6/shutil.py\", line 486, in rmtree\n    _rmtree_safe_fd(fd, path, onerror)\n  File \"/opt/ibm/conda/miniconda36/lib/python3.6/shutil.py\", line 424, in _rmtree_safe_fd\n    _rmtree_safe_fd(dirfd, fullname, onerror)\n  File \"/opt/ibm/conda/miniconda36/lib/python3.6/shutil.py\", line 444, in _rmtree_safe_fd\n    onerror(os.unlink, fullname, sys.exc_info())\n  File \"/opt/ibm/conda/miniconda36/lib/python3.6/shutil.py\", line 442, in _rmtree_safe_fd\n    os.unlink(name, dir_fd=topfd)\nOSError: [Errno 16] Device or resource busy: '.nfs00000000028595eb00000153'\u001b[0m\n"
                }, 
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Collecting SciPy\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/4c/5f81e7264b0a7a8bd570810f48cd346ba36faedbd2ba255c873ad556de76/scipy-1.3.0-cp36-cp36m-manylinux1_x86_64.whl (25.2MB)\n\u001b[K    100% |################################| 25.2MB 67.6MB/s ta 0:00:01\n\u001b[?25hCollecting numpy>=1.13.3 (from SciPy)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/b9/bda9781f0a74b90ebd2e046fde1196182900bd4a8e1ea503d3ffebc50e7c/numpy-1.17.0-cp36-cp36m-manylinux1_x86_64.whl (20.4MB)\n\u001b[K    100% |################################| 20.4MB 72.5MB/s ta 0:00:01\n\u001b[31mtensorflow 1.13.1 requires tensorboard<1.14.0,>=1.13.0, which is not installed.\u001b[0m\n\u001b[?25hInstalling collected packages: numpy, SciPy\nSuccessfully installed SciPy-1.3.0 numpy-1.17.0\n\u001b[31mException:\nTraceback (most recent call last):\n  File \"/opt/ibm/conda/miniconda36/lib/python3.6/site-packages/pip/_internal/cli/base_command.py\", line 176, in main\n    status = self.run(options, args)\n  File \"/opt/ibm/conda/miniconda36/lib/python3.6/site-packages/pip/_internal/commands/install.py\", line 441, in run\n    options.target_dir, target_temp_dir, options.upgrade\n  File \"/opt/ibm/conda/miniconda36/lib/python3.6/site-packages/pip/_internal/commands/install.py\", line 492, in _handle_target_dir\n    shutil.rmtree(target_item_dir)\n  File \"/opt/ibm/conda/miniconda36/lib/python3.6/shutil.py\", line 486, in rmtree\n    _rmtree_safe_fd(fd, path, onerror)\n  File \"/opt/ibm/conda/miniconda36/lib/python3.6/shutil.py\", line 424, in _rmtree_safe_fd\n    _rmtree_safe_fd(dirfd, fullname, onerror)\n  File \"/opt/ibm/conda/miniconda36/lib/python3.6/shutil.py\", line 444, in _rmtree_safe_fd\n    onerror(os.unlink, fullname, sys.exc_info())\n  File \"/opt/ibm/conda/miniconda36/lib/python3.6/shutil.py\", line 442, in _rmtree_safe_fd\n    os.unlink(name, dir_fd=topfd)\nOSError: [Errno 16] Device or resource busy: '.nfs00000000028595eb00000153'\u001b[0m\nCollecting imblearn\n  Using cached https://files.pythonhosted.org/packages/81/a7/4179e6ebfd654bd0eac0b9c06125b8b4c96a9d0a8ff9e9507eb2a26d2d7e/imblearn-0.0-py2.py3-none-any.whl\nCollecting imbalanced-learn (from imblearn)\n  Using cached https://files.pythonhosted.org/packages/e6/62/08c14224a7e242df2cef7b312d2ef821c3931ec9b015ff93bb52ec8a10a3/imbalanced_learn-0.5.0-py3-none-any.whl\nCollecting scikit-learn>=0.21 (from imbalanced-learn->imblearn)\n  Using cached https://files.pythonhosted.org/packages/a0/c5/d2238762d780dde84a20b8c761f563fe882b88c5a5fb03c056547c442a19/scikit_learn-0.21.3-cp36-cp36m-manylinux1_x86_64.whl\nCollecting scipy>=0.17 (from imbalanced-learn->imblearn)\n  Using cached https://files.pythonhosted.org/packages/72/4c/5f81e7264b0a7a8bd570810f48cd346ba36faedbd2ba255c873ad556de76/scipy-1.3.0-cp36-cp36m-manylinux1_x86_64.whl\nCollecting joblib>=0.11 (from imbalanced-learn->imblearn)\n  Using cached https://files.pythonhosted.org/packages/cd/c1/50a758e8247561e58cb87305b1e90b171b8c767b15b12a1734001f41d356/joblib-0.13.2-py2.py3-none-any.whl\nCollecting numpy>=1.11 (from imbalanced-learn->imblearn)\n  Using cached https://files.pythonhosted.org/packages/19/b9/bda9781f0a74b90ebd2e046fde1196182900bd4a8e1ea503d3ffebc50e7c/numpy-1.17.0-cp36-cp36m-manylinux1_x86_64.whl\n\u001b[31mtensorflow 1.13.1 requires tensorboard<1.14.0,>=1.13.0, which is not installed.\u001b[0m\nInstalling collected packages: numpy, joblib, scipy, scikit-learn, imbalanced-learn, imblearn\nSuccessfully installed imbalanced-learn-0.5.0 imblearn-0.0 joblib-0.13.2 numpy-1.17.0 scikit-learn-0.21.3 scipy-1.3.0\n\u001b[31mException:\nTraceback (most recent call last):\n  File \"/opt/ibm/conda/miniconda36/lib/python3.6/site-packages/pip/_internal/cli/base_command.py\", line 176, in main\n    status = self.run(options, args)\n  File \"/opt/ibm/conda/miniconda36/lib/python3.6/site-packages/pip/_internal/commands/install.py\", line 441, in run\n    options.target_dir, target_temp_dir, options.upgrade\n  File \"/opt/ibm/conda/miniconda36/lib/python3.6/site-packages/pip/_internal/commands/install.py\", line 492, in _handle_target_dir\n    shutil.rmtree(target_item_dir)\n  File \"/opt/ibm/conda/miniconda36/lib/python3.6/shutil.py\", line 486, in rmtree\n    _rmtree_safe_fd(fd, path, onerror)\n  File \"/opt/ibm/conda/miniconda36/lib/python3.6/shutil.py\", line 424, in _rmtree_safe_fd\n    _rmtree_safe_fd(dirfd, fullname, onerror)\n  File \"/opt/ibm/conda/miniconda36/lib/python3.6/shutil.py\", line 444, in _rmtree_safe_fd\n    onerror(os.unlink, fullname, sys.exc_info())\n  File \"/opt/ibm/conda/miniconda36/lib/python3.6/shutil.py\", line 442, in _rmtree_safe_fd\n    os.unlink(name, dir_fd=topfd)\nOSError: [Errno 16] Device or resource busy: '.nfs00000000028595eb00000153'\u001b[0m\n"
                }
            ], 
            "execution_count": 2
        }, 
        {
            "source": "!pip install plotly --upgrade\n!pip install chart-studio --upgrade\n\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Collecting plotly\n  Using cached https://files.pythonhosted.org/packages/58/f3/a49d3281cc7275164ecf89ad3497556b11d9661faa119becdf7f9d3b2125/plotly-4.0.0-py2.py3-none-any.whl\nCollecting retrying>=1.3.3 (from plotly)\nCollecting six (from plotly)\n  Using cached https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\n\u001b[31mtensorflow 1.13.1 requires tensorboard<1.14.0,>=1.13.0, which is not installed.\u001b[0m\n\u001b[31mibm-cos-sdk-core 2.5.2 has requirement urllib3<1.25,>=1.20, but you'll have urllib3 1.25.3 which is incompatible.\u001b[0m\n\u001b[31mbotocore 1.12.82 has requirement urllib3<1.25,>=1.20, but you'll have urllib3 1.25.3 which is incompatible.\u001b[0m\nInstalling collected packages: six, retrying, plotly\nSuccessfully installed plotly-4.0.0 retrying-1.3.3 six-1.12.0\nCollecting chart-studio\n  Using cached https://files.pythonhosted.org/packages/b9/3f/d2f3f506ba1aaf109f549f8b01d1483cd3e324c5ebe6b206acee66efdf46/chart_studio-1.0.0-py3-none-any.whl\nCollecting six (from chart-studio)\n  Using cached https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\nCollecting requests (from chart-studio)\n  Using cached https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl\nCollecting retrying>=1.3.3 (from chart-studio)\nCollecting plotly (from chart-studio)\n  Using cached https://files.pythonhosted.org/packages/58/f3/a49d3281cc7275164ecf89ad3497556b11d9661faa119becdf7f9d3b2125/plotly-4.0.0-py2.py3-none-any.whl\nCollecting certifi>=2017.4.17 (from requests->chart-studio)\n  Using cached https://files.pythonhosted.org/packages/69/1b/b853c7a9d4f6a6d00749e94eb6f3a041e342a885b87340b79c1ef73e3a78/certifi-2019.6.16-py2.py3-none-any.whl\nCollecting idna<2.9,>=2.5 (from requests->chart-studio)\n  Using cached https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl\nCollecting chardet<3.1.0,>=3.0.2 (from requests->chart-studio)\n  Using cached https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl\nCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests->chart-studio)\n  Using cached https://files.pythonhosted.org/packages/e6/60/247f23a7121ae632d62811ba7f273d0e58972d75e58a94d329d51550a47d/urllib3-1.25.3-py2.py3-none-any.whl\n\u001b[31mtensorflow 1.13.1 requires tensorboard<1.14.0,>=1.13.0, which is not installed.\u001b[0m\n\u001b[31mspyder 3.3.3 requires pyqt5<=5.12; python_version >= \"3\", which is not installed.\u001b[0m\n\u001b[31mibm-cos-sdk-core 2.5.2 has requirement urllib3<1.25,>=1.20, but you'll have urllib3 1.25.3 which is incompatible.\u001b[0m\n\u001b[31mbotocore 1.12.82 has requirement urllib3<1.25,>=1.20, but you'll have urllib3 1.25.3 which is incompatible.\u001b[0m\nInstalling collected packages: six, certifi, idna, chardet, urllib3, requests, retrying, plotly, chart-studio\nSuccessfully installed certifi-2019.6.16 chardet-3.0.4 chart-studio-1.0.0 idna-2.8 plotly-4.0.0 requests-2.22.0 retrying-1.3.3 six-1.12.0 urllib3-1.25.3\n"
                }
            ], 
            "execution_count": 3
        }, 
        {
            "source": "## Define Watson Machine Learning Credentials", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "WML_CREDENTIALS = {\n  \"apikey\": \"haikTOuwibItbgRYtDwuTD1Nnht8bIN96LeoSHihp4Db\",\n  \"iam_apikey_description\": \"Auto generated apikey during resource-key operation for Instance - crn:v1:bluemix:public:pm-20:us-south:a/97deeb0b7e78431438a00a04f20580b7:8385ac85-436d-470e-8e67-54c492adb554::\",\n  \"iam_apikey_name\": \"auto-generated-apikey-d7945c16-a7df-479f-84e8-56a1ea40416b\",\n  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Writer\",\n  \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/97deeb0b7e78431438a00a04f20580b7::serviceid:ServiceId-c35b5b3e-030f-4cde-9edc-9f234768fe85\",\n  \"instance_id\": \"8385ac85-436d-470e-8e67-54c492adb554\",\n  \"password\": \"8726f331-3271-4456-b593-e90675da02aa\",\n  \"url\": \"https://us-south.ml.cloud.ibm.com\",\n  \"username\": \"d7945c16-a7df-479f-84e8-56a1ea40416b\"\n}", 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "execution_count": 4
        }, 
        {
            "source": "## Load the training data from Object Storage and Import required libraries", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "import chart_studio.plotly as py\nimport plotly.graph_objs as go\nimport types\nimport pandas as pd\nfrom botocore.client import Config\nimport ibm_boto3\nimport numpy as np\nimport numpy.dual as dual\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.over_sampling import SMOTENC\n\n\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer, IndexToString, VectorAssembler\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml import Pipeline, Model\nfrom pyspark.ml import linalg\n\nimport types\nimport pandas as pd\nfrom botocore.client import Config\nimport ibm_boto3\n\ndef __iter__(self): return 0\n\n# @hidden_cell\n# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n# You might want to remove those credentials before you share your notebook.\nclient_e2f5521e5ce34913a7133599cf5a489b = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id='ZGGx-3CVX-Or7x5q8ebThDz8Xz9Js8kxCIlcamsSHUYo',\n    ibm_auth_endpoint=\"https://iam.ng.bluemix.net/oidc/token\",\n    config=Config(signature_version='oauth'),\n    endpoint_url='https://s3-api.us-geo.objectstorage.service.networklayer.com')\n\nbody = client_e2f5521e5ce34913a7133599cf5a489b.get_object(Bucket='equipmentfailure3-donotdelete-pr-2hqbpzzieda7iq',Key='EQUIPMENT_FAILURE.csv')['Body']\n# add missing __iter__ method, so pandas accepts body as file-like object\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\npd_data = pd.read_csv(body)\npd_data.head()\n\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "\n# @hidden_cell\n# The following code contains the credentials for a file in your IBM Cloud Object Storage.\n# You might want to remove those credentials before you share your notebook.\ncredentials = {\n    'IAM_SERVICE_ID': 'iam-ServiceId-695cac76-4f4e-4493-b816-9402b9a49a22',\n    'IBM_API_KEY_ID': 'm82at4NJKaQl3liTTeYsLIpZHBJLGW_CfAbmCG4xsd09',\n    'ENDPOINT': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n    'IBM_AUTH_ENDPOINT': 'https://iam.ng.bluemix.net/oidc/token',\n    'BUCKET': 'newdemonstration-donotdelete-pr-ysh8ivjgootiwj',\n    'FILE': 'predictive_maintenance.csv'\n}\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "## Explore the original Data", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "pd_data.isnull().sum(axis = 0)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "pd_data.shape", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "pd_data.columns", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "tips_summed = pd_data.groupby(['EQUIPMENT_FAILURE'])['S1'].count()\ntips_summed", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "pd_data=pd_data.sort_values(by=['WELL_ID', 'DATE'], ascending=[True, False])\npd_data.reset_index(level=0, inplace=True)\n\npd_data.head(10)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "## Check and remove Duplicate Rows", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "df_failure_thingy=pd_data\ndf_failure_thingy=df_failure_thingy.drop_duplicates(subset=['WELL_ID','DATE'])\ndf_failure_thingy.shape\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "duplicateRowsDF = pd_data[pd_data.duplicated(['DATE','WELL_ID'])]\nduplicateRowsDF", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "pd_data=pd_data.drop_duplicates(subset=['WELL_ID','DATE'])\npd_data.shape", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "## Data transformations and Feature Engineering", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### Convert dates from character to dates", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "pd_data['DATE'] = pd.to_datetime(pd_data['DATE'])\n\n\n\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#### Identify a change in id with a new field called flipper.  Also, identify the change in days between each record", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "pd_data=pd_data.sort_values(by=['WELL_ID','DATE'], ascending=[True, True])\n\npd_data['yesterday'] =(pd_data.DATE.shift(1))\npd_data['C'] = pd_data['DATE'] - pd_data['yesterday']\n\npd_data['TIME'] = pd_data['C'] / np.timedelta64(1, 'D')\npd_data['zz'] = np.where(((pd_data.TIME > 1) & ((pd_data.WELL_ID == pd_data.WELL_ID.shift(1)))), 1, 0)\npd_data['flipper'] = np.where((pd_data.WELL_ID != pd_data.WELL_ID.shift(1)), 1, 0)\n\npd_data['zzz'] = np.where(((pd_data.zz ==1) | (pd_data.flipper==1)), 1, 0)\npd_data.head(10)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "pd_data.isnull().sum(axis = 0)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#### Create a running mean, median, max and min.  Also flag fields that do not have at least five days of history to calculate the running values", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "pd_data['SUM_zzz']=pd_data['zzz'].rolling(min_periods=1, window=5).sum()\n\npd_data['S1_mean'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S1'].rolling(min_periods=1, window=5).mean()) , -99999999)\npd_data['S1_median'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S1'].rolling(min_periods=1, window=5).median()) , -99999999)\npd_data['S1_max'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S1'].rolling(min_periods=1, window=5).max()) , -99999999)\npd_data['S1_min'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S1'].rolling(min_periods=1, window=5).min()) , -99999999)\n\n\n\npd_data['SUM_zzz']=pd_data['zzz'].rolling(min_periods=1, window=5).sum()\n\npd_data['S2_mean'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S2'].rolling(min_periods=1, window=5).mean()) , -99999999)\npd_data['S2_median'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S2'].rolling(min_periods=1, window=5).median()) , -99999999)\npd_data['S2_max'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S2'].rolling(min_periods=1, window=5).max()) , -99999999)\npd_data['S2_min'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S2'].rolling(min_periods=1, window=5).min()) , -99999999)\n\n\npd_data['S3_mean'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S3'].rolling(min_periods=1, window=5).mean()) , -99999999)\npd_data['S3_median'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S3'].rolling(min_periods=1, window=5).median()) , -99999999)\npd_data['S3_max'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S3'].rolling(min_periods=1, window=5).max()) , -99999999)\npd_data['S3_min'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S3'].rolling(min_periods=1, window=5).min()) , -99999999)\n\n\npd_data['S4_mean'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S4'].rolling(min_periods=1, window=5).mean()) , -99999999)\npd_data['S4_median'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S4'].rolling(min_periods=1, window=5).median()) , -99999999)\npd_data['S4_max'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S4'].rolling(min_periods=1, window=5).max()) , -99999999)\npd_data['S4_min'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S4'].rolling(min_periods=1, window=5).min()) , -99999999)\n\npd_data['S5_mean'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S5'].rolling(min_periods=1, window=5).mean()) , -99999999)\npd_data['S5_median'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S5'].rolling(min_periods=1, window=5).median()) , -99999999)\npd_data['S5_max'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S5'].rolling(min_periods=1, window=5).max()) , -99999999)\npd_data['S5_min'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S5'].rolling(min_periods=1, window=5).min()) , -99999999)\n\npd_data['S6_mean'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S6'].rolling(min_periods=1, window=5).mean()) , -99999999)\npd_data['S6_median'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S6'].rolling(min_periods=1, window=5).median()) , -99999999)\npd_data['S6_max'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S6'].rolling(min_periods=1, window=5).max()) , -99999999)\npd_data['S6_min'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S6'].rolling(min_periods=1, window=5).min()) , -99999999)\n\npd_data['S7_mean'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S7'].rolling(min_periods=1, window=5).mean()) , -99999999)\npd_data['S7_median'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S7'].rolling(min_periods=1, window=5).median()) , -99999999)\npd_data['S7_max'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S7'].rolling(min_periods=1, window=5).max()) , -99999999)\npd_data['S7_min'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S7'].rolling(min_periods=1, window=5).min()) , -99999999)\n\npd_data['S8_mean'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S8'].rolling(min_periods=1, window=5).mean()) , -99999999)\npd_data['S8_median'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S8'].rolling(min_periods=1, window=5).median()) , -99999999)\npd_data['S8_max'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S8'].rolling(min_periods=1, window=5).max()) , -99999999)\npd_data['S8_min'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S8'].rolling(min_periods=1, window=5).min()) , -99999999)\n\npd_data['S9_mean'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S9'].rolling(min_periods=1, window=5).mean()) , -99999999)\npd_data['S9_median'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S9'].rolling(min_periods=1, window=5).median()) , -99999999)\npd_data['S9_max'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S9'].rolling(min_periods=1, window=5).max()) , -99999999)\npd_data['S9_min'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S9'].rolling(min_periods=1, window=5).min()) , -99999999)\n\npd_data['S10_mean'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S10'].rolling(min_periods=1, window=5).mean()) , -99999999)\npd_data['S10_median'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S10'].rolling(min_periods=1, window=5).median()) , -99999999)\npd_data['S10_max'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S10'].rolling(min_periods=1, window=5).max()) , -99999999)\npd_data['S10_min'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S10'].rolling(min_periods=1, window=5).min()) , -99999999)\n\npd_data['S11_mean'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S11'].rolling(min_periods=1, window=5).mean()) , -99999999)\npd_data['S11_median'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S11'].rolling(min_periods=1, window=5).median()) , -99999999)\npd_data['S11_max'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S11'].rolling(min_periods=1, window=5).max()) , -99999999)\npd_data['S11_min'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S11'].rolling(min_periods=1, window=5).min()) , -99999999)\n\n\npd_data['S12_mean'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S12'].rolling(min_periods=1, window=5).mean()) , -99999999)\npd_data['S12_median'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S12'].rolling(min_periods=1, window=5).median()) , -99999999)\npd_data['S12_max'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S12'].rolling(min_periods=1, window=5).max()) , -99999999)\npd_data['S12_min'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S12'].rolling(min_periods=1, window=5).min()) , -99999999)\n\npd_data['S13_mean'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S13'].rolling(min_periods=1, window=5).mean()) , -99999999)\npd_data['S13_median'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S13'].rolling(min_periods=1, window=5).median()) , -99999999)\npd_data['S13_max'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S13'].rolling(min_periods=1, window=5).max()) , -99999999)\npd_data['S13_min'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S13'].rolling(min_periods=1, window=5).min()) , -99999999)\n\npd_data['S14_mean'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S14'].rolling(min_periods=1, window=5).mean()) , -99999999)\npd_data['S14_median'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S14'].rolling(min_periods=1, window=5).median()) , -99999999)\npd_data['S14_max'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S14'].rolling(min_periods=1, window=5).max()) , -99999999)\npd_data['S14_min'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S14'].rolling(min_periods=1, window=5).min()) , -99999999)\n\npd_data['S15_mean'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S15'].rolling(min_periods=1, window=5).mean()) , -99999999)\npd_data['S15_median'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S15'].rolling(min_periods=1, window=5).median()) , -99999999)\npd_data['S15_max'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S15'].rolling(min_periods=1, window=5).max()) , -99999999)\npd_data['S15_min'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S15'].rolling(min_periods=1, window=5).min()) , -99999999)\n\npd_data['S16_mean'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S16'].rolling(min_periods=1, window=5).mean()) , -99999999)\npd_data['S16_median'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S16'].rolling(min_periods=1, window=5).median()) , -99999999)\npd_data['S16_max'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S16'].rolling(min_periods=1, window=5).max()) , -99999999)\npd_data['S16_min'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S16'].rolling(min_periods=1, window=5).min()) , -99999999)\n\npd_data['S17_mean'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S17'].rolling(min_periods=1, window=5).mean()) , -99999999)\npd_data['S17_median'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S17'].rolling(min_periods=1, window=5).median()) , -99999999)\npd_data['S17_max'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S17'].rolling(min_periods=1, window=5).max()) , -99999999)\npd_data['S17_min'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S17'].rolling(min_periods=1, window=5).min()) , -99999999)\n\npd_data['S18_mean'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S18'].rolling(min_periods=1, window=5).mean()) , -99999999)\npd_data['S18_median'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S18'].rolling(min_periods=1, window=5).median()) , -99999999)\npd_data['S18_max'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S18'].rolling(min_periods=1, window=5).max()) , -99999999)\npd_data['S18_min'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S18'].rolling(min_periods=1, window=5).min()) , -99999999)\n\npd_data['S19_mean'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S19'].rolling(min_periods=1, window=5).mean()) , -99999999)\npd_data['S19_median'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S19'].rolling(min_periods=1, window=5).median()) , -99999999)\npd_data['S19_max'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S19'].rolling(min_periods=1, window=5).max()) , -99999999)\npd_data['S19_min'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S19'].rolling(min_periods=1, window=5).min()) , -99999999)\n\npd_data['S20_mean'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S20'].rolling(min_periods=1, window=5).mean()) , -99999999)\npd_data['S20_median'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S20'].rolling(min_periods=1, window=5).median()) , -99999999)\npd_data['S20_max'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S20'].rolling(min_periods=1, window=5).max()) , -99999999)\npd_data['S20_min'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S20'].rolling(min_periods=1, window=5).min()) , -99999999)\n\npd_data['S21_mean'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S21'].rolling(min_periods=1, window=5).mean()) , -99999999)\npd_data['S21_median'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S21'].rolling(min_periods=1, window=5).median()) , -99999999)\npd_data['S21_max'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S21'].rolling(min_periods=1, window=5).max()) , -99999999)\npd_data['S21_min'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S21'].rolling(min_periods=1, window=5).min()) , -99999999)\n\npd_data['S22_mean'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S22'].rolling(min_periods=1, window=5).mean()) , -99999999)\npd_data['S22_median'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S22'].rolling(min_periods=1, window=5).median()) , -99999999)\npd_data['S22_max'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S22'].rolling(min_periods=1, window=5).max()) , -99999999)\npd_data['S22_min'] = np.where((pd_data.SUM_zzz == 0),(pd_data['S22'].rolling(min_periods=1, window=5).min()) , -99999999)\n\n\n\n\n\n\n\n\n\n\n\n\npd_data.head(1)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "pd_data.isnull().sum(axis = 0)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#### Create a poor man's derivative for each signal", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "pd_data['S1_chg'] = np.where((pd_data.S1_mean == 0),0 , pd_data.S1/pd_data.S1_mean)\npd_data['S2_chg'] = np.where((pd_data.S2_mean == 0),0 , pd_data.S2/pd_data.S2_mean)\npd_data['S3_chg'] = np.where((pd_data.S3_mean == 0),0 , pd_data.S3/pd_data.S3_mean)\npd_data['S4_chg'] = np.where((pd_data.S4_mean == 0),0 , pd_data.S4/pd_data.S4_mean)\npd_data['S5_chg'] = np.where((pd_data.S5_mean == 0),0 , pd_data.S5/pd_data.S5_mean)\npd_data['S6_chg'] = np.where((pd_data.S6_mean == 0),0 , pd_data.S6/pd_data.S6_mean)\npd_data['S7_chg'] = np.where((pd_data.S7_mean == 0),0 , pd_data.S7/pd_data.S7_mean)\npd_data['S8_chg'] = np.where((pd_data.S8_mean == 0),0 , pd_data.S8/pd_data.S8_mean)\npd_data['S9_chg'] = np.where((pd_data.S9_mean == 0),0 , pd_data.S9/pd_data.S9_mean)\npd_data['S10_chg'] = np.where((pd_data.S10_mean == 0),0 , pd_data.S10/pd_data.S10_mean)\npd_data['S11_chg'] = np.where((pd_data.S11_mean == 0),0 , pd_data.S11/pd_data.S11_mean)\npd_data['S12_chg'] = np.where((pd_data.S12_mean == 0),0 , pd_data.S12/pd_data.S12_mean)\npd_data['S13_chg'] = np.where((pd_data.S13_mean == 0),0 , pd_data.S13/pd_data.S13_mean)\npd_data['S14_chg'] = np.where((pd_data.S14_mean == 0),0 , pd_data.S14/pd_data.S14_mean)\npd_data['S15_chg'] = np.where((pd_data.S15_mean == 0),0 , pd_data.S15/pd_data.S15_mean)\npd_data['S16_chg'] = np.where((pd_data.S16_mean == 0),0 , pd_data.S16/pd_data.S16_mean)\npd_data['S17_chg'] = np.where((pd_data.S17_mean == 0),0 , pd_data.S17/pd_data.S17_mean)\npd_data['S18_chg'] = np.where((pd_data.S18_mean == 0),0 , pd_data.S18/pd_data.S18_mean)\npd_data['S19_chg'] = np.where((pd_data.S19_mean == 0),0 , pd_data.S19/pd_data.S19_mean)\npd_data['S20_chg'] = np.where((pd_data.S20_mean == 0),0 , pd_data.S20/pd_data.S20_mean)\npd_data['S21_chg'] = np.where((pd_data.S20_mean == 0),0 , pd_data.S21/pd_data.S21_mean)\npd_data['S22_chg'] = np.where((pd_data.S20_mean == 0),0 , pd_data.S22/pd_data.S22_mean)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "pd_data.isnull().sum(axis = 0)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#### Append the failure date for each id", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "pd_data['S21_chg'].fillna((pd_data['S21_chg'].mean()), inplace=True)\npd_data['S22_chg'].fillna((pd_data['S22_chg'].mean()), inplace=True)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "pd_data.isnull().sum(axis = 0)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "pd_data=pd_data.sort_values(by=['WELL_ID', 'DATE'], ascending=[True, True])\npd_data.reset_index(level=0, inplace=True)\n\npd_data.head(10)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "df_failure_thingy=pd_data[pd_data['EQUIPMENT_FAILURE'] == 1]\n\ndf_failure_thingy=df_failure_thingy[['DATE','WELL_ID']]\n\ndf_failure_thingy=df_failure_thingy.rename(index=str, columns={\"DATE\": \"FAILURE_DATE\"})\n\ndf_failure_thingy.shape", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "pd_data=pd_data.sort_values(by=['WELL_ID'], ascending=[True])\ndf_failure_thingy=df_failure_thingy.sort_values(by=['WELL_ID'], ascending=[True])", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "pd_data =pd_data.merge(df_failure_thingy, on=['WELL_ID'], how='left')\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#### Create some descriptive stats of the new feature engineered variables", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "pd_data.isnull().sum(axis = 0)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "pd_data.describe()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "### Create a new target variable that represents the 30 days before an actual failure", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### Use the Failure date of each machine to calculate the Time to Failure", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "pd_data=pd_data.sort_values(by=['WELL_ID','DATE'], ascending=[True, True])\n\npd_data['FAILURE_DATE'] = pd.to_datetime(pd_data['FAILURE_DATE'])\npd_data['DATE'] = pd.to_datetime(pd_data['DATE'])\npd_data['C'] = pd_data['FAILURE_DATE'] - pd_data['DATE']\n\npd_data['TIME_TO_FAILURE'] = pd_data['C'] / np.timedelta64(1, 'D')\n\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "pd_data=pd_data.sort_values(by=['WELL_ID', 'DATE'], ascending=[True, True])", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "pd_data.describe()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "pd.set_option('display.max_rows', 50000)\n", 
            "cell_type": "raw", 
            "metadata": {}
        }, 
        {
            "source": "#### Set the failure window", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "pd_data['FAILURE_TARGET'] = np.where(((pd_data.TIME_TO_FAILURE < 30) & ((pd_data.TIME_TO_FAILURE>=0))), 1, 0)\n\npd_data.head(10)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#### Count the new failure target", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "tips_summed = pd_data.groupby(['FAILURE_TARGET'])['S1'].count()\ntips_summed", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#### Eliminate records that don't have at least 5 days of history", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "pd_data=pd_data[pd_data['S1_min'] > -99999999]", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#### Examine the new data set", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "pd_data.describe()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#### Now we have 12513 target observations", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "tips_summed = pd_data.groupby(['FAILURE_TARGET'])['S1'].count()\ntips_summed", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "## Randomly assign each id to a modeling, testing or a validation category.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### Get a unique set of ids", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "pd_well_id=pd_data.drop_duplicates(subset='WELL_ID')\npd_well_id=pd_well_id[['WELL_ID']]\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "pd_well_id.head(5)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "pd_well_id.shape", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#### Create a new variable with a random number between 0 and 1", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "pd_well_id['wookie'] = (np.random.randint(0, 10000, pd_well_id.shape[0]))/10000", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "pd_well_id['wookiex'] = (np.random.randint(0, 10000, pd_well_id.shape[0]))/10000", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "pd_well_id=pd_well_id[pd_well_id['wookiex'] <0.50]\npd_well_id=pd_well_id[['WELL_ID', 'wookie']]", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "pd_well_id.columns", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#### Give each record a 35% chance of being in the validation, a 30% chance of being in the testing and a 35% chance of being in the modeling", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "pd_well_id['MODELING_GROUP'] = np.where(((pd_well_id.wookie <= 0.350)), 'MODELING', np.where(((pd_well_id.wookie <= 0.65)), 'TESTING', 'VALIDATION'))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "tips_summed = pd_well_id.groupby(['MODELING_GROUP'])['wookie'].count()\ntips_summed", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#### Append the Group of each id to each individual record", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "pd_data=pd_data.sort_values(by=['WELL_ID'], ascending=[True])\npd_well_id=pd_well_id.sort_values(by=['WELL_ID'], ascending=[True])", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "pd_data =pd_data.merge(pd_well_id, on=['WELL_ID'], how='left')\n\npd_data.head()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "tips_summed = pd_data.groupby(['MODELING_GROUP'])['wookie'].count()\ntips_summed", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "tips_summed = pd_data.groupby(['MODELING_GROUP'])['EQUIPMENT_FAILURE'].sum()\ntips_summed", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "pd_data.head()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "## Prep data to build a machine learning model using SPARK", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### Create a separate dataframe from each modeling group", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "df_validation=pd_data[pd_data['MODELING_GROUP'] == 'VALIDATION']\n\n\ndf_validation=df_validation.drop(columns=['MODELING_GROUP','wookie','TIME_TO_FAILURE','FAILURE_DATE','TIME','yesterday','zz','flipper','zzz','SUM_zzz','level_0','index'])\ndf_validation.shape", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "df_modeling=pd_data[pd_data['MODELING_GROUP'] == 'MODELING']\n\ndf_modeling=df_modeling.drop(columns=['MODELING_GROUP','EQUIPMENT_FAILURE','C','wookie','TIME_TO_FAILURE','FAILURE_DATE','TIME','yesterday','zz','flipper','zzz','SUM_zzz','level_0','index'])\ndf_modeling.shape", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "df_totalz=pd_data.drop(columns=['C','wookie','TIME_TO_FAILURE','FAILURE_DATE','TIME','yesterday','zz','flipper','zzz','SUM_zzz','level_0','index'])\ndf_totalz.shape", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "df_testing=pd_data[pd_data['MODELING_GROUP'] == 'TESTING']\n\ndf_testing=df_testing.drop(columns=['MODELING_GROUP','C','wookie','TIME_TO_FAILURE','FAILURE_DATE','TIME','yesterday','zz','flipper','zzz','SUM_zzz','level_0','index'])\ndf_testing_original=df_testing", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "pd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "cols = df_modeling.columns.tolist()\ncols", 
            "cell_type": "raw", 
            "metadata": {}
        }, 
        {
            "source": "#### Define the Training features and Target", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "training_features=df_modeling[['REGION_CLUSTER',\n 'MAINTENANCE_VENDOR',\n 'MANUFACTURER',\n 'WELL_GROUP',\n 'TIME_SINCE_MAINTENANCE',\n 'S1',\n 'S2',\n 'S3',\n 'S4',\n 'S5',\n 'S6',\n 'S7',\n 'S8',\n 'S9',\n 'S10',\n 'S11',\n 'S12',\n 'S13',\n 'S14',\n 'S15',\n 'S16',\n 'S17',\n 'S18',\n 'S19',\n 'S20',\n 'S21',\n 'S22',\n 'S1_mean',\n 'S1_median',\n 'S1_max',\n 'S1_min',\n 'S2_mean',\n 'S2_median',\n 'S2_max',\n 'S2_min',\n 'S3_mean',\n 'S3_median',\n 'S3_max',\n 'S3_min',\n 'S4_mean',\n 'S4_median',\n 'S4_max',\n 'S4_min',\n 'S5_mean',\n 'S5_median',\n 'S5_max',\n 'S5_min',\n 'S6_mean',\n 'S6_median',\n 'S6_max',\n 'S6_min',\n 'S7_mean',\n 'S7_median',\n 'S7_max',\n 'S7_min',\n 'S8_mean',\n 'S8_median',\n 'S8_max',\n 'S8_min',\n 'S9_mean',\n 'S9_median',\n 'S9_max',\n 'S9_min',\n 'S10_mean',\n 'S10_median',\n 'S10_max',\n 'S10_min',\n 'S11_mean',\n 'S11_median',\n 'S11_max',\n 'S11_min',\n 'S12_mean',\n 'S12_median',\n 'S12_max',\n 'S12_min',\n 'S13_mean',\n 'S13_median',\n 'S13_max',\n 'S13_min',\n 'S14_mean',\n 'S14_median',\n 'S14_max',\n 'S14_min',\n 'S15_mean',\n 'S15_median',\n 'S15_max',\n 'S15_min',\n 'S16_mean',\n 'S16_median',\n 'S16_max',\n 'S16_min',\n 'S17_mean',\n 'S17_median',\n 'S17_max',\n 'S17_min',\n 'S18_mean',\n 'S18_median',\n 'S18_max',\n 'S18_min',\n 'S19_mean',\n 'S19_median',\n 'S19_max',\n 'S19_min',\n 'S20_mean',\n 'S20_median',\n 'S20_max',\n 'S20_min',\n 'S21_mean',\n 'S21_median',\n 'S21_max',\n 'S21_min',\n 'S22_mean',\n 'S22_median',\n 'S22_max',\n 'S22_min']]\ntesting_features=df_validation[['REGION_CLUSTER',\n 'MAINTENANCE_VENDOR',\n 'MANUFACTURER',\n 'WELL_GROUP',\n 'TIME_SINCE_MAINTENANCE',\n 'S1',\n 'S2',\n 'S3',\n 'S4',\n 'S5',\n 'S6',\n 'S7',\n 'S8',\n 'S9',\n 'S10',\n 'S11',\n 'S12',\n 'S13',\n 'S14',\n 'S15',\n 'S16',\n 'S17',\n 'S18',\n 'S19',\n 'S20',\n 'S21',\n 'S22',\n 'S1_mean',\n 'S1_median',\n 'S1_max',\n 'S1_min',\n 'S2_mean',\n 'S2_median',\n 'S2_max',\n 'S2_min',\n 'S3_mean',\n 'S3_median',\n 'S3_max',\n 'S3_min',\n 'S4_mean',\n 'S4_median',\n 'S4_max',\n 'S4_min',\n 'S5_mean',\n 'S5_median',\n 'S5_max',\n 'S5_min',\n 'S6_mean',\n 'S6_median',\n 'S6_max',\n 'S6_min',\n 'S7_mean',\n 'S7_median',\n 'S7_max',\n 'S7_min',\n 'S8_mean',\n 'S8_median',\n 'S8_max',\n 'S8_min',\n 'S9_mean',\n 'S9_median',\n 'S9_max',\n 'S9_min',\n 'S10_mean',\n 'S10_median',\n 'S10_max',\n 'S10_min',\n 'S11_mean',\n 'S11_median',\n 'S11_max',\n 'S11_min',\n 'S12_mean',\n 'S12_median',\n 'S12_max',\n 'S12_min',\n 'S13_mean',\n 'S13_median',\n 'S13_max',\n 'S13_min',\n 'S14_mean',\n 'S14_median',\n 'S14_max',\n 'S14_min',\n 'S15_mean',\n 'S15_median',\n 'S15_max',\n 'S15_min',\n 'S16_mean',\n 'S16_median',\n 'S16_max',\n 'S16_min',\n 'S17_mean',\n 'S17_median',\n 'S17_max',\n 'S17_min',\n 'S18_mean',\n 'S18_median',\n 'S18_max',\n 'S18_min',\n 'S19_mean',\n 'S19_median',\n 'S19_max',\n 'S19_min',\n 'S20_mean',\n 'S20_median',\n 'S20_max',\n 'S20_min',\n 'S21_mean',\n 'S21_median',\n 'S21_max',\n 'S21_min',\n 'S22_mean',\n 'S22_median',\n 'S22_max',\n 'S22_min']]", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "training_target=df_modeling[['FAILURE_TARGET']]\ntesting_target=df_validation[['FAILURE_TARGET']]\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#### Synthetically Balance the training and testing data sets with a smote algorithm", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "import numpy.dual as dual", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "from imblearn.over_sampling import SMOTE\nfrom imblearn.over_sampling import SMOTENC\nsm = SMOTE(random_state=12, ratio = 1.0)\nsmx = SMOTENC(random_state=12,  categorical_features=[0, 1, 2, 3])", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "x_res, y_res = smx.fit_sample(training_features, training_target.values.ravel())", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "x1_res, y1_res = smx.fit_sample(testing_features, testing_target.values.ravel())", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#### Convert the SMOTE output back to complete dataframes with independent and dependent variables.  Examine the results", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "df_x=pd.DataFrame(x_res)\n\ndf_x.columns = ['REGION_CLUSTER',\n 'MAINTENANCE_VENDOR',\n 'MANUFACTURER',\n 'WELL_GROUP',\n 'TIME_SINCE_MAINTENANCE',\n 'S1',\n 'S2',\n 'S3',\n 'S4',\n 'S5',\n 'S6',\n 'S7',\n 'S8',\n 'S9',\n 'S10',\n 'S11',\n 'S12',\n 'S13',\n 'S14',\n 'S15',\n 'S16',\n 'S17',\n 'S18',\n 'S19',\n 'S20',\n 'S21',\n 'S22',\n 'S1_mean',\n 'S1_median',\n 'S1_max',\n 'S1_min',\n 'S2_mean',\n 'S2_median',\n 'S2_max',\n 'S2_min',\n 'S3_mean',\n 'S3_median',\n 'S3_max',\n 'S3_min',\n 'S4_mean',\n 'S4_median',\n 'S4_max',\n 'S4_min',\n 'S5_mean',\n 'S5_median',\n 'S5_max',\n 'S5_min',\n 'S6_mean',\n 'S6_median',\n 'S6_max',\n 'S6_min',\n 'S7_mean',\n 'S7_median',\n 'S7_max',\n 'S7_min',\n 'S8_mean',\n 'S8_median',\n 'S8_max',\n 'S8_min',\n 'S9_mean',\n 'S9_median',\n 'S9_max',\n 'S9_min',\n 'S10_mean',\n 'S10_median',\n 'S10_max',\n 'S10_min',\n 'S11_mean',\n 'S11_median',\n 'S11_max',\n 'S11_min',\n 'S12_mean',\n 'S12_median',\n 'S12_max',\n 'S12_min',\n 'S13_mean',\n 'S13_median',\n 'S13_max',\n 'S13_min',\n 'S14_mean',\n 'S14_median',\n 'S14_max',\n 'S14_min',\n 'S15_mean',\n 'S15_median',\n 'S15_max',\n 'S15_min',\n 'S16_mean',\n 'S16_median',\n 'S16_max',\n 'S16_min',\n 'S17_mean',\n 'S17_median',\n 'S17_max',\n 'S17_min',\n 'S18_mean',\n 'S18_median',\n 'S18_max',\n 'S18_min',\n 'S19_mean',\n 'S19_median',\n 'S19_max',\n 'S19_min',\n 'S20_mean',\n 'S20_median',\n 'S20_max',\n 'S20_min',\n 'S21_mean',\n 'S21_median',\n 'S21_max',\n 'S21_min',\n 'S22_mean',\n 'S22_median',\n 'S22_max',\n 'S22_min']\ndf_x.head()\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "df1_x=pd.DataFrame(x1_res)\n\ndf1_x.columns = ['REGION_CLUSTER',\n 'MAINTENANCE_VENDOR',\n 'MANUFACTURER',\n 'WELL_GROUP',\n 'TIME_SINCE_MAINTENANCE',\n 'S1',\n 'S2',\n 'S3',\n 'S4',\n 'S5',\n 'S6',\n 'S7',\n 'S8',\n 'S9',\n 'S10',\n 'S11',\n 'S12',\n 'S13',\n 'S14',\n 'S15',\n 'S16',\n 'S17',\n 'S18',\n 'S19',\n 'S20',\n 'S21',\n 'S22',\n 'S1_mean',\n 'S1_median',\n 'S1_max',\n 'S1_min',\n 'S2_mean',\n 'S2_median',\n 'S2_max',\n 'S2_min',\n 'S3_mean',\n 'S3_median',\n 'S3_max',\n 'S3_min',\n 'S4_mean',\n 'S4_median',\n 'S4_max',\n 'S4_min',\n 'S5_mean',\n 'S5_median',\n 'S5_max',\n 'S5_min',\n 'S6_mean',\n 'S6_median',\n 'S6_max',\n 'S6_min',\n 'S7_mean',\n 'S7_median',\n 'S7_max',\n 'S7_min',\n 'S8_mean',\n 'S8_median',\n 'S8_max',\n 'S8_min',\n 'S9_mean',\n 'S9_median',\n 'S9_max',\n 'S9_min',\n 'S10_mean',\n 'S10_median',\n 'S10_max',\n 'S10_min',\n 'S11_mean',\n 'S11_median',\n 'S11_max',\n 'S11_min',\n 'S12_mean',\n 'S12_median',\n 'S12_max',\n 'S12_min',\n 'S13_mean',\n 'S13_median',\n 'S13_max',\n 'S13_min',\n 'S14_mean',\n 'S14_median',\n 'S14_max',\n 'S14_min',\n 'S15_mean',\n 'S15_median',\n 'S15_max',\n 'S15_min',\n 'S16_mean',\n 'S16_median',\n 'S16_max',\n 'S16_min',\n 'S17_mean',\n 'S17_median',\n 'S17_max',\n 'S17_min',\n 'S18_mean',\n 'S18_median',\n 'S18_max',\n 'S18_min',\n 'S19_mean',\n 'S19_median',\n 'S19_max',\n 'S19_min',\n 'S20_mean',\n 'S20_median',\n 'S20_max',\n 'S20_min',\n 'S21_mean',\n 'S21_median',\n 'S21_max',\n 'S21_min',\n 'S22_mean',\n 'S22_median',\n 'S22_max',\n 'S22_min']\ndf1_x.head()\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "df_x.shape", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "df1_x.shape", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "df_y=pd.DataFrame(y_res)\ndf_y.columns = ['FAILURE_TARGET']\n\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "df1_y=pd.DataFrame(y1_res)\ndf1_y.columns = ['FAILURE_TARGET']\n\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "df_y.shape", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "df1_y.shape", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "df_y.mean(axis = 0) ", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "df1_y.mean(axis = 0) ", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "df_modeling = pd.concat([df_y, df_x], axis=1)\ndf_testing = pd.concat([df1_y, df1_x], axis=1)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "df_modeling.head()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "df_testing.head()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#### Convert the Pandas Dataframes to SPARK Dataframes", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "df_modelingx = spark.createDataFrame(df_modeling)\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "df_validationx=spark.createDataFrame(df_validation)\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "df_testing_originalx=spark.createDataFrame(df_testing_original)\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "df_testingx=spark.createDataFrame(df_testing)\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "print(\"Number of records: \" + str(df_modelingx.count()))", 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "print(\"Number of records: \" + str(df_validationx.count()))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "print(\"Number of records: \" + str(df_testingx.count()))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "print(\"Number of records: \" + str(df_testing_originalx.count()))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "print(\"Number of records: \" + str(df_totalx.count()))", 
            "cell_type": "raw", 
            "metadata": {}
        }, 
        {
            "source": "# Create a SPARK Machine Learning Model", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### Define the training or modeling data", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "spark_df = df_modelingx\n\ntrain_data=spark_df\ntest_data=df_testingx\n\nMODEL_NAME = \"Equipment Failure Model\"\nDEPLOYMENT_NAME = \"Equipment Failure Model\"\n\nprint(\"Number of records for training: \" + str(train_data.count()))\nprint(\"Number of records for evaluation: \" + str(test_data.count()))\n\ntrain_data.printSchema()", 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#### Define the labels or dependent variable\n\n", 
            "cell_type": "markdown", 
            "metadata": {
                "scrolled": true
            }
        }, 
        {
            "source": "si_Label = StringIndexer(inputCol=\"FAILURE_TARGET\", outputCol=\"label\").fit(train_data)\nlabel_converter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=si_Label.labels)", 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#### Define the features or independent variables", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "si_REGION_CLUSTER = StringIndexer(inputCol = 'REGION_CLUSTER', outputCol = 'REGION_CLUSTER_IX')\nsi_MAINTENANCE_VENDOR = StringIndexer(inputCol = 'MAINTENANCE_VENDOR', outputCol = 'MAINTENANCE_VENDOR_IX')\nsi_MANUFACTURER = StringIndexer(inputCol = 'MANUFACTURER', outputCol = 'MANUFACTURER_IX')\nsi_WELL_GROUP = StringIndexer(inputCol = 'WELL_GROUP', outputCol = 'WELL_GROUP_IX')", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "va_features = VectorAssembler(inputCols=['REGION_CLUSTER_IX', 'MAINTENANCE_VENDOR_IX',\n       'MANUFACTURER_IX', 'WELL_GROUP_IX', 'TIME_SINCE_MAINTENANCE', 'S1', 'S2',\n       'S3', 'S4', 'S5', 'S6', 'S7', 'S8', 'S9', 'S10', 'S11', 'S12', 'S13',\n       'S14', 'S15', 'S16', 'S17', 'S18', 'S19', 'S20', 'S21', 'S22'], outputCol=\"features\")\n\n\nva_features", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "va_features = VectorAssembler(inputCols=['REGION_CLUSTER_IX',\n 'MAINTENANCE_VENDOR_IX',\n 'MANUFACTURER_IX',\n 'WELL_GROUP_IX',\n 'TIME_SINCE_MAINTENANCE',\n 'S1',\n 'S2',\n 'S3',\n 'S4',\n 'S5',\n 'S6',\n 'S7',\n 'S8',\n 'S9',\n 'S10',\n 'S11',\n 'S12',\n 'S13',\n 'S14',\n 'S15',\n 'S16',\n 'S17',\n 'S18',\n 'S19',\n 'S20',\n 'S21',\n 'S22',\n 'S1_mean',\n 'S1_median',\n 'S1_max',\n 'S1_min',\n 'S2_mean',\n 'S2_median',\n 'S2_max',\n 'S2_min',\n 'S3_mean',\n 'S3_median',\n 'S3_max',\n 'S3_min',\n 'S4_mean',\n 'S4_median',\n 'S4_max',\n 'S4_min',\n 'S5_mean',\n 'S5_median',\n 'S5_max',\n 'S5_min',\n 'S6_mean',\n 'S6_median',\n 'S6_max',\n 'S6_min',\n 'S7_mean',\n 'S7_median',\n 'S7_max',\n 'S7_min',\n 'S8_mean',\n 'S8_median',\n 'S8_max',\n 'S8_min',\n 'S9_mean',\n 'S9_median',\n 'S9_max',\n 'S9_min',\n 'S10_mean',\n 'S10_median',\n 'S10_max',\n 'S10_min',\n 'S11_mean',\n 'S11_median',\n 'S11_max',\n 'S11_min',\n 'S12_mean',\n 'S12_median',\n 'S12_max',\n 'S12_min',\n 'S13_mean',\n 'S13_median',\n 'S13_max',\n 'S13_min',\n 'S14_mean',\n 'S14_median',\n 'S14_max',\n 'S14_min',\n 'S15_mean',\n 'S15_median',\n 'S15_max',\n 'S15_min',\n 'S16_mean',\n 'S16_median',\n 'S16_max',\n 'S16_min',\n 'S17_mean',\n 'S17_median',\n 'S17_max',\n 'S17_min',\n 'S18_mean',\n 'S18_median',\n 'S18_max',\n 'S18_min',\n 'S19_mean',\n 'S19_median',\n 'S19_max',\n 'S19_min',\n 'S20_mean',\n 'S20_median',\n 'S20_max',\n 'S20_min',\n 'S21_mean',\n 'S21_median',\n 'S21_max',\n 'S21_min',\n 'S22_mean',\n 'S22_median',\n 'S22_max',\n 'S22_min'], outputCol=\"features\")", 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#### Define the classifier or algorithm.  Define the pipeline and build the model", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from pyspark.ml.classification import RandomForestClassifier\nclassifier = RandomForestClassifier(featuresCol=\"features\")\nfrom pyspark.ml.classification import GBTClassifier\n#classifier = GBTClassifier(featuresCol=\"features\")\n\npipeline = Pipeline(stages=[si_Label, si_REGION_CLUSTER, si_MAINTENANCE_VENDOR, si_MANUFACTURER, si_WELL_GROUP, va_features, classifier, label_converter])\n", 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "model = pipeline.fit(train_data)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "train_data.head()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#### Apply the model to the training data and estimate an area under the ROC metric", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "pred_train = model.transform(train_data)\nevaluatorDT = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\")\narea_under_curve = evaluatorDT.evaluate(pred_train)\n\n#default evaluation is areaUnderROC\nprint(\"areaUnderROC = %g\" % area_under_curve)", 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "####  Apply the model to the testing data and estimate an area under the ROC metric", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "pred_test = model.transform(test_data)\nevaluatorDT = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\")\narea_under_curve = evaluatorDT.evaluate(pred_test)\n\n#default evaluation is areaUnderROC\nprint(\"areaUnderROC = %g\" % area_under_curve)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#### Apply the model to the original testing data and estimate an area under the ROC metric", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "pred_test_original = model.transform(df_testing_originalx)\nevaluatorDT = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\")\narea_under_curve = evaluatorDT.evaluate(pred_test_original)\n\n#default evaluation is areaUnderROC\nprint(\"areaUnderROC = %g\" % area_under_curve)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#### Apply the model to the validation data and estimate an area under the ROC metric", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "pred_valid = model.transform(df_validationx)\nevaluatorDT = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\")\narea_under_curve = evaluatorDT.evaluate(pred_valid)\n\n#default evaluation is areaUnderROC\nprint(\"areaUnderROC = %g\" % area_under_curve)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "pred_total = model.transform(df_totalx)\nevaluatorDT = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\")\narea_under_curve = evaluatorDT.evaluate(pred_total)\n\n#default evaluation is areaUnderROC\nprint(\"areaUnderROC = %g\" % area_under_curve)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "## Examine the validation results with a confusion matrix", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### convert the scored validation data to a Pandas dataframe", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "df_output=pred_valid.toPandas() ", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#### create a numeric predicted variable", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "df_output=df_output.sort_values(by=['id', 'date'], ascending=[True, True])", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "df=df_output\n\ndf['P_EQUIPMENT_FAILURE'] = pd.to_numeric(df['predictedLabel'], errors='coerce')\n\n", 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "df_output.head()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#### Create the cross-tab with the Target variable", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "df['bootie']=1", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "table = pd.pivot_table(df, values=['bootie'], index=['MODELING_GROUP','FAILURE_TARGET'],columns=['P_EQUIPMENT_FAILURE'], aggfunc=np.sum)\ntable", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#### Create the cross-tab with the actual failure indicator", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "df.columns", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "table = pd.pivot_table(df, values=['bootie'], index=['MODELING_GROUP','failure'],columns=['P_EQUIPMENT_FAILURE'], aggfunc=np.sum)\ntable", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "xxxx=df[df['id'] == 'S1F0GG8X']\nxxxx", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "## Create and apply hueristics to make the predicted variable more realisitic.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### A prediction is only a prediction if the previous value is also a prediction", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "df['FLATTENED_PREDICTION']=np.where((df.P_EQUIPMENT_FAILURE.shift(1)+ df.P_EQUIPMENT_FAILURE ==2) & df.P_EQUIPMENT_FAILURE==1 ,1,0)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "df=df.sort_values(by=['id', 'date'], ascending=[True, True])", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#### Identify the relevant failure date (if there is one) for each record", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "df_failure_thingy=df[df['failure'] == 1]\ndf_failure_thingy.head()\n\ndf_failure_thingy=df_failure_thingy[['date','id']]\n\ndf_failure_thingy=df_failure_thingy.rename(index=str, columns={\"date\": \"FAILURE_DATE\"})", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "df_failure_thingy=df_failure_thingy.drop_duplicates(subset='id')\ndf_failure_thingy.shape", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "df=df.sort_values(by=['id'], ascending=[True])\ndf_failure_thingy=df_failure_thingy.sort_values(by=['id'], ascending=[True])\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "df =df.merge(df_failure_thingy, on=['id'], how='outer')", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#### Calculate the time to failure for each record.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "df['FAILURE_DATE'] = pd.to_datetime(df['FAILURE_DATE'])\ndf['date'] = pd.to_datetime(df['date'])\ndf['C'] = df['FAILURE_DATE'] - df['date']\n\ndf['TIME_TO_FAILURE'] = df['C'] / np.timedelta64(1, 'D')", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#### For each id -- identify the first date and the last date in the data sample", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "df_dates=df\n\ndf_dates=df_dates[['date','id']]\n\n\nMAX_DATE= pd.DataFrame(df.groupby(['id'])['date'].max())\nMIN_DATE= pd.DataFrame(df.groupby(['id'])['date'].min())\n\nMAX_DATE=MAX_DATE.rename(index=str, columns={\"date\": \"MAX_DATE\"})\nMIN_DATE=MIN_DATE.rename(index=str, columns={\"date\": \"MIN_DATE\"})\n\nMAX_MIN_DATE =MIN_DATE.merge(MAX_DATE, on=['id'], how='outer')\nMAX_MIN_DATE.reset_index(level=0, inplace=True)\n\nMAX_MIN_DATE=MAX_MIN_DATE.drop_duplicates(subset=['id'])\n\ndf =df.merge(MAX_MIN_DATE, on=['id'], how='inner')\ndf.shape", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "df=df.sort_values(by=['id','date'], ascending=[True,True])", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#### a prediction is a prediction of failure if and only if the previous previous prediciton is a non-failure.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "df['FLATTENED_PREDICTION_2']=np.where((df.FLATTENED_PREDICTION.shift(1)==1) &(df.FLATTENED_PREDICTION==1),0,\n                                      (np.where((df.FLATTENED_PREDICTION.shift(1)==1) &(df.FLATTENED_PREDICTION==0),0,\n                                                (np.where((df.FLATTENED_PREDICTION.shift(1)==0) &(df.FLATTENED_PREDICTION==0),0,\n                                                          (np.where((df.FLATTENED_PREDICTION.shift(1)==0) &(df.FLATTENED_PREDICTION==1),1,0)))))))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "counts = df['FLATTENED_PREDICTION_2'].value_counts()\ncounts\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "counts = df['failure'].value_counts()\ncounts", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#### create a unique id for each signal in the data set", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "df['SIGNAL_ID'] = df['FLATTENED_PREDICTION_2'].cumsum()\n#cumulative sums of groups to column des\ndf['TIME_SINCE_SIGNAL']= df.groupby(['SIGNAL_ID'])['bootie'].cumsum()\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "df_signals=df[df['FLATTENED_PREDICTION_2'] == 1]\ndf_signal_date=df_signals[['SIGNAL_ID','date','id']]\ndf_signal_date=df_signal_date.rename(index=str, columns={\"date\": \"SIGNAL_DATE\"})\ndf_signal_date=df_signal_date.rename(index=str, columns={\"id\": \"ID_OF_SIGNAL\"})\n\n\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "df =df.merge(df_signal_date, on=['SIGNAL_ID'], how='outer')\n", 
            "cell_type": "code", 
            "metadata": {
                "scrolled": false
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "df=df[['date', 'id', 'failure', 'FAILURE_TARGET',\n       'P_EQUIPMENT_FAILURE', 'FLATTENED_PREDICTION', 'FAILURE_DATE',\n       'TIME_TO_FAILURE', 'MIN_DATE', 'MAX_DATE',\n       'FLATTENED_PREDICTION_2', 'SIGNAL_ID',\n       'SIGNAL_DATE','ID_OF_SIGNAL','MODELING_GROUP']]\n\n\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#### create a field called Warning that indicates the time since a signal of failure", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "df['C'] = df['FAILURE_DATE'] - df['SIGNAL_DATE']\ndf['WARNING'] = df['C'] / np.timedelta64(1, 'D')", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#### Now Definte a True Positive, True Negative, False Positive and False Negative\n\nA true positive occurs if and only if the machine fails and there was a signal within the last 200 days.\n\n\nA false negative occurs if and only if the machine fails and there is no signal within the previous 200 days.\n\n\n\nA False Positive occurs if there is a failure signal and a failure does not  occur in the next 200 days.\n\nIf an observation is not a False Positive, a False Negative or a True Positive it is a True Negative.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "\ndf['TRUE_POSITIVE'] = np.where(((df.failure == 1) & (df.WARNING<=200) & (df.ID_OF_SIGNAL==df.id)), 1, 0)\ndf['FALSE_NEGATIVE'] = np.where((df.TRUE_POSITIVE==0) & (df.failure==1), 1, 0)\n\ndf['FALSE_POSITIVE'] = np.where(((df.FLATTENED_PREDICTION_2 == 1) & (df.WARNING>=200) & (df.ID_OF_SIGNAL==df.id)), 1, 0)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "df['bootie']=1", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#### Create the final Cross-Tab", 
            "cell_type": "markdown", 
            "metadata": {
                "scrolled": false
            }
        }, 
        {
            "source": "def func(row):\n    if row['FALSE_POSITIVE'] == 1:\n        return 'FALSE_POSITIVE'\n    elif row['FALSE_NEGATIVE'] == 1:\n        return 'FALSE_NEGATIVE' \n    elif row['TRUE_POSITIVE'] == 1:\n        return 'TRUE_POSITIVE' \n    else:\n        return 'TRUE_NEGATIVE'\n\ndf['CATEGORY'] = df.apply(func, axis=1)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "table = pd.pivot_table(df, values=['bootie'], index=['MODELING_GROUP'],columns=['CATEGORY'], aggfunc=np.sum)\ntable", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#### Clean up and write output to object storage", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "df_for_export=df[['date','id','CATEGORY','FLATTENED_PREDICTION_2']]\ndf_stuff=df_output[['date','id','probability','P_EQUIPMENT_FAILURE','FLATTENED_PREDICTION','FAILURE_TARGET','MODELING_GROUP','failure']]", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "df_for_export =df_stuff.merge(df_for_export, on=['date','id'], how='inner')\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "df_for_export.shape", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "from ibm_botocore.client import Config\nimport ibm_boto3\ncos = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id=credentials['IBM_API_KEY_ID'],\n    ibm_service_instance_id=credentials['IAM_SERVICE_ID'],\n    ibm_auth_endpoint=credentials['IBM_AUTH_ENDPOINT'],\n    config=Config(signature_version='oauth'),\n    endpoint_url=credentials['ENDPOINT'])", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "df_for_export=df_for_export.to_csv('df_for_export.csv',index=False)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "cos.upload_file(Filename='df_for_export.csv',Bucket=credentials['BUCKET'],Key='df_for_export.csv')", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6 with Spark", 
            "name": "python36", 
            "language": "python3"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.6.8", 
            "name": "python", 
            "pygments_lexer": "ipython3", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}